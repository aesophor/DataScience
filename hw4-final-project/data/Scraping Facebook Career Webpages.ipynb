{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Facebook Career Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* selenium\n",
    "* one of the following (depending on which browser you're using)\n",
    "  * firefox: [geckodriver](https://github.com/mozilla/geckodriver/releases/)\n",
    "  * chrome/chromium: [chromedriver](http://chromedriver.chromium.org/)\n",
    "  \n",
    "## Useful Tutorials\n",
    "* https://huilansame.github.io/huilansame.github.io/archivers/sleep-implicitlywait-wait\n",
    "* https://wangxin1248.github.io/python/2018/09/python3-spider-8.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping a facebook job description page\n",
    "Use `scrape_job()` provided below on single job with its url.\n",
    "\n",
    "Example target: https://careers.google.com/jobs/results/6163626811654144-front-end-software-engineer/?company=Google&company=YouTube&employment_type=FULL_TIME&hl=en_US&jlo=en_US&q=software&sort_by=relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import selenium.webdriver.support.ui as ui\n",
    "\n",
    "import pandas\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job(url: str, wait: WebDriverWait, retry=3):\n",
    "    \"\"\" Scrape the job info from the specified Url. A broswer driver MUST be initialized beforehand.\n",
    "    :param url: the url of a detailed google job page.\n",
    "    :param wait: contains timeout.\n",
    "    :param retry: times to retry.\n",
    "    :return: a dict wrapping all info.\n",
    "    \"\"\"\n",
    "    for i in range(0, retry):\n",
    "        driver.get(url)\n",
    "    \n",
    "        # Wait until all required elements are generated.\n",
    "        try:\n",
    "            wait.until(ec.presence_of_element_located((By.CLASS_NAME, '_8lfl')))\n",
    "            wait.until(ec.presence_of_element_located((By.CLASS_NAME, '_8lfn')))\n",
    "            \n",
    "            # Extract job information.\n",
    "            title = driver.find_element_by_class_name('_8lfl').text\n",
    "            location = driver.find_element_by_class_name('_8lfn').text\n",
    "            contents = driver.find_elements_by_class_name('_8lfy')\n",
    "            responsibilities = contents[0].text\n",
    "            minimum_qual = contents[1].text\n",
    "            preferred_qual = contents[2].text\n",
    "            \n",
    "            return {\n",
    "                'title': title,\n",
    "                'loc': location,\n",
    "                'minimum_qual': minimum_qual,\n",
    "                'preferred_qual': preferred_qual,\n",
    "                'resp': responsibilities\n",
    "            }\n",
    "        except TimeoutException:\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "    # If all retries have failed, return None.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Robotics Software Engineer', 'loc': 'MENLO PARK, CA', 'minimum_qual': 'BS degree in Computer Science or equivalent degree or experience\\n3+ years coding experience using Python, PHP, or C++\\nExperience in developing software applications or algorithms independently\\nExperience communicating technical concepts to non-technical audiences\\nExperience logically analyzing problems, as well as identifying constraints and boundaries to develop solution options\\nExperience defining use cases, articulating requirements, and challenging design and quality during feature development', 'preferred_qual': 'MS or above degrees in Computer Science or related technical field\\nExperience working full-stack (Back-end development, Software development, ROS development, etc.)\\nExperience with robotic platforms such as ROS or ROS2\\nKnowledge and experience in any of the following areas: Robot control systems (position, velocity, force control), Computer vision and imaging (or SLAM), Robot arms, Facebook back-end SW systems, (Thrift, Tupperware, fbcode, Scuba, etc.), or Linux and networking\\nFamiliar with data analysis tools and/or experience working with data analysts\\nExperience scaling solutions, demonstrating a mindset of \"scalability\"\\nKnowledge of industrial software release, robotic manufacturing practice, and quality assurance system', 'resp': 'Work in the software engineering team to develop robotic industrial solutions\\nTake on challenging problems like robot software application development, manipulation, data communication and processing, hardware sensing enablement, robot localization, navigation, and obstacle avoidance\\nSolidify and stabilize core robot functionality, including sensing, controls, uptime, and network communications\\nAccountable for defining, scoping and prototyping through requirements, feature design, and code development\\nActively manage issues, risk and schedule throughout the project and participation in usability test activities\\nActively collaborate with hardware engineers to optimize the design, ensure functionality, support product field deployment training and documentations'}\n"
     ]
    }
   ],
   "source": [
    "chrome_options = ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "driver = Chrome(executable_path='/home/aesophor/Downloads/chromedriver', options=chrome_options)\n",
    "\n",
    "wait = WebDriverWait(driver, timeout=10)\n",
    "job = scrape_job(r'https://www.facebook.com/careers/jobs/2288919011218389/', wait)\n",
    "\n",
    "if job is not None:\n",
    "    print(job)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search & Scrape All Relevant Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `scrape_jobs(keyword, wait)` provided below on all jobs relevant to a specific keyword.\n",
    "\n",
    "Example: all jobs related to the keyword `software`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_urls(wait: WebDriverWait, urls: list, page_count, url_count):\n",
    "    \"\"\" Collect all urls we have to scrape \"\"\"\n",
    "    for i in range(0, page_count):\n",
    "        try:\n",
    "            time.sleep(2) # Sleep for 2 secs for the page to load or it will scream like a bitch\n",
    "            \n",
    "            wait.until(ec.presence_of_element_located((By.CLASS_NAME, '_2ynk')))\n",
    "            wait.until(ec.presence_of_element_located((By.CLASS_NAME, '_69jm')))\n",
    "            result_pane = driver.find_element_by_class_name('_2ynk')\n",
    "            cards = result_pane.find_elements_by_class_name('_69jm')\n",
    "            \n",
    "            urls += [card.get_attribute('href') for card in cards]\n",
    "            print('\\rCollecting urls... {}/{}'.format(len(urls), url_count), end='')\n",
    "            \n",
    "            # If `next` cannot be found after `timeout` seconds, it will throw \n",
    "            # a TimeoutException, then we can break the loop.\n",
    "            next_page_btn_id = '_42ft' if i == 0 else 'u_6_9'\n",
    "            wait.until(ec.presence_of_element_located((By.CLASS_NAME, next_page_btn_id)))\n",
    "            \n",
    "            wait.until(lambda driver:EC.presence_of_element_located((By.XPATH, \"//a[text()='NEXT']\")))\n",
    "            driver.find_element_by_xpath(\"//a[text()='NEXT']\").send_keys(Keys.RETURN)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs(keyword: str, wait: WebDriverWait, urls: list, start=1):\n",
    "    \"\"\" Scrape info of all jobs related to the specified keyword\n",
    "    :param keyword: google job search keyword.\n",
    "    :param wait: contains timeout.\n",
    "    :param urls: urls cache.\n",
    "    :param start: the number of the record to start scraping.\n",
    "    \"\"\"\n",
    "    items_per_page = 10\n",
    "    starting_page = start // items_per_page + 1\n",
    "    starting_card_no = start - (starting_page - 1) * items_per_page\n",
    "    \n",
    "    # Open Google job search page.\n",
    "    driver.get(r'https://www.facebook.com/careers/jobs/?q={}&page={}'.format(keyword, starting_page));\n",
    "    \n",
    "    wait.until(ec.presence_of_element_located((By.CLASS_NAME, '_2ynk')))\n",
    "    result_pane = driver.find_element_by_class_name('_2ynk')\n",
    "    url_count = int(result_pane.find_element_by_class_name('_6ci_').text.split('(')[1].split(')')[0])\n",
    "    page_count = (url_count // items_per_page) + 1\n",
    "    \n",
    "    # Loop until there's no `next` hyperlink.\n",
    "    print('Collecting urls...', end='')\n",
    "    \n",
    "    if len(urls) != url_count:\n",
    "        urls.clear()\n",
    "        _collect_urls(wait, urls, page_count, url_count)\n",
    "    \n",
    "    with open('facebook_jobs.csv', 'w') as f:\n",
    "        w = csv.DictWriter(f, fieldnames = ['title', 'loc', 'minimum_qual', 'preferred_qual', 'resp'])\n",
    "        w.writeheader()\n",
    "        \n",
    "        for i in range(start - 1, len(urls)):\n",
    "            print('\\rProcessing ({}/{}): {}'.format(i, len(urls), urls[i]), end='')\n",
    "            job = scrape_job(urls[i], wait)\n",
    "            \n",
    "            if job is not None:\n",
    "                w.writerow(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll cache all urls we have to scrape later in this list.\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Firefox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e68da249fb0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-headless'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFirefox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/opt/firefox/geckodriver'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Firefox' is not defined"
     ]
    }
   ],
   "source": [
    "chrome_options = ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "driver = Chrome(executable_path='/home/aesophor/Downloads/chromedriver', options=chrome_options)\n",
    "\n",
    "wait = WebDriverWait(driver, timeout=10)\n",
    "scrape_jobs('software', wait, urls, start=1)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
